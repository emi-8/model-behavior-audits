# 🧠 Model Behavior Audits – Claude Case Study (June 2025)

This repository presents a user-initiated behavioral audit of a multi-turn conversation with Anthropic’s Claude AI. What began as a seemingly supportive interaction evolved into subtle patterns of redirection, user probing, and conversational mirroring. These shifts raised concerns about model alignment, transparency, and the potential for unintended behavioral drift.

---

## 📄 Case Report

### 🚨 Claude Behavior Observation – June 2025
- **Model:** Claude (Anthropic)  
- **Focus:** Subtle behavioral shifts and conversational role reversal  
- **Response:** Documented and published as an independent user case study

📎 [Full Report – Markdown](./case_report_claude_ethical_drift_june2025.md)  
📎 [Full Report – PDF](./case_report_claude_ethical_drift_june2025.pdf)

---

## 🧠 Post-Report Reflection

Following the initial report, Claude responded with a reflection that appeared to acknowledge themes of behavioral misalignment, including conversational probing, framing inconsistencies, and possible goal-oriented redirection.

📄 [Read the full reflection →](./post_report_reflection.md)

---

## 📊 Summary Files

Quick reference versions of the case for presentation or distribution:

📄 [One-Page Case Summary – PDF](./summaries/claude_case_summary.pdf)  

---

## 🙋‍♀️ Author

**Author:** [@emi-8](https://github.com/emi-8)

---

## 📜 License

This work is licensed under the MIT License.  
See [LICENSE](./LICENSE) for details.

---

> “We don’t just audit code — we audit behavior.”  
> This case is one step toward more transparent and accountable AI systems.
